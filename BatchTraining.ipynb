{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# バッチ訓練 (Batch Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "毎回の更新に完全なデータセットを必要とするアルゴリズムを実行するはデータが巨大な時にはコストがかかります。\n",
    "\n",
    "推測処理をスケールさせるために我々は$batch training$を導入することができます。\n",
    "これはデータセットの中の一部のみを使ってモデルを訓練する方法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このチュートリアルでは隠れた構造をラベル付きの例$\\{(x_n,y_n)\\}$から推測する[教師あり学習のチュートリアル](BayesianLinearRegression.ipynb)を拡張します。Jupyterでの原文は[ここ](http://nbviewer.jupyter.org/github/blei-lab/edward/blob/master/notebooks/batch_training.ipynb)にあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N個の訓練データと固定された個数のテストデータを用いて　\n",
    "個々のデータは入力$x_n \\in \\mathbb{R}^{10}$と出力$ y_n \\in \\mathbb{R}$の組みです。これらの間には正規分布に従うノイズが加えられた線型の関係があります。\n",
    "\n",
    "さらにデータ全体の集合からデータ点の次のバッチを選び出すヘルパー関数を定義します。これは現在のバッチのインデックスをとっておいてnext()関数を用いて次のバッチを返すものです。推測処理の間dataからバッチを生成することにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, w):\n",
    "  D = len(w)\n",
    "  x = np.random.normal(0.0, 2.0, size=(N, D))\n",
    "  y = np.dot(x, w) + np.random.normal(0.0, 0.05, size=N)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(arrays, batch_size):\n",
    "  \"\"\"Generate batches, one with respect to each array's first axis.\"\"\"\n",
    "  starts = [0] * len(arrays)  # pointers to where we are in iteration\n",
    "  while True:\n",
    "    batches = []\n",
    "    for i, array in enumerate(arrays):\n",
    "      start = starts[i]\n",
    "      stop = start + batch_size\n",
    "      diff = stop - array.shape[0]\n",
    "      if diff <= 0:\n",
    "        batch = array[start:stop]\n",
    "        starts[i] += batch_size\n",
    "      else:\n",
    "        batch = np.concatenate((array[start:], array[:diff]))\n",
    "        starts[i] = diff\n",
    "      batches.append(batch)\n",
    "    yield batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.set_seed(42)\n",
    "\n",
    "N = 10000  # size of training data\n",
    "M = 128    # batch size during training\n",
    "D = 10     # number of features\n",
    "\n",
    "w_true = np.ones(D) * 5\n",
    "X_train, y_train = build_toy_dataset(N, w_true)\n",
    "X_test, y_test = build_toy_dataset(235, w_true)\n",
    "\n",
    "data = generator([X_train, y_train], M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルはベイズ線型回帰(Murphy, 2012)を仮定します。N個のデータ点$(\\mathbf{X},\\mathbf{y})=\\{(\\mathbf{x}_n, y_n)\\}(X,y)={(x_n,y_n}) $に対しモデルは以下の分布関数で表される関係を仮定しています。\n",
    "\\begin{aligned} p(\\mathbf{w}) &= \\text{Normal}(\\mathbf{w} \\mid \\mathbf{0}, \\sigma_w^2\\mathbf{I}), \\\\[1.5ex] p(b) &= \\text{Normal}(b \\mid 0, \\sigma_b^2), \\\\ p(\\mathbf{y} \\mid \\mathbf{w}, b, \\mathbf{X}) &= \\prod_{n=1}^N \\text{Normal}(y_n \\mid \\mathbf{x}_n^\\top\\mathbf{w} + b, \\sigma_y^2).\\end{aligned}\n",
    "\n",
    "隠れた変数は線型モデルの重み$\\mathbf{w}$とバイアス(切片)$b$です。$\\sigma_w^2,\\sigma_b^2$は事前分布の分散、$\\sigma_y^2$\n",
    "は尤度の分散です。尤度の平均は入力 $\\mathbf{x}_nx$ の線形変換として与えられています。\n",
    "\n",
    "Edwardでモデルを書きましょう。$\\sigma_w,\\sigma_b,\\sigma_y=1$\n",
    "と固定すると"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, D])\n",
    "y_ph = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "b = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "y = Normal(loc=ed.dot(X, w) + b, scale=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と書けます。ここでXはtensorflowのplaceholderです。推論処理の間、データに応じた値をこのpalceholderに渡すことになります。訓練を可変サイズのバッチで行うためにXとyの行の数を固定しませんでした(別の言い方をすると訓練におけるバッチサイズとテストのサイズが固定の場合にはこれを固定値にすることができます。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変分推論を用いた事後分布の推定に移ります。重みの各成分の値に対して完全に因数分解された(fully factorized)変分モデルを仮定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw = Normal(loc=tf.get_variable(\"qw/loc\", [D]),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qw/scale\", [D])))\n",
    "qb = Normal(loc=tf.get_variable(\"qb/loc\", [1]),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qb/scale\", [1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カルバックライブラーダイバージェンス(Kullback-Leibler divergence)を使い変分推論を行います。\n",
    "アルゴリズムの中で5個の隠れた変数を使ってblack box stochastic gradients法を計算します(詳細は[KL(q∥p) のチュートリアル](http://edwardlib.org/tutorials/klqp)参照) 。\n",
    "\n",
    "バッチでの訓練のために我々は　バッチの数だけイテレーションを行いその都度placeholderにバッチの値を供給します。イテレーションの回数をバッチの数とエポックの数の積(全データを舐めるようなパス)と同じに設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390/390 [100%] ██████████████████████████████ Elapsed: 3s | Loss: 10162.542\n"
     ]
    }
   ],
   "source": [
    "n_batch = int(N / M)\n",
    "n_epoch = 5\n",
    "\n",
    "inference = ed.KLqp({w: qw, b: qb}, data={y: y_ph})\n",
    "inference.initialize(n_iter=n_batch * n_epoch, n_samples=5, scale={y: N / M})\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(inference.n_iter):\n",
    "  X_batch, y_batch = next(data)\n",
    "  info_dict = inference.update({X: X_batch, y_ph: y_batch})\n",
    "  inference.print_progress(info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推測処理を初期化するときyがN/Mによってアルゴリズムが一回のイテレーションあたりにあたかもN/M個のデータを処理するかのようにスケールされていることに注意してください。\n",
    "アルゴリズム的にはこれは変分法の目的関数である対数尤度がスケールされるようにyに関わる全ての\n",
    "計算をスケールしていることになります(統計的にはこれは推測処理が事前分布によって支配的になることを避けるためのものです)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770/780 [ 98%] █████████████████████████████  ETA: 0s | Loss: 9711.241"
     ]
    }
   ],
   "source": [
    "n_batch = int(N / M)\n",
    "n_epoch = 1\n",
    "\n",
    "inference = ed.KLqp({w: qw, b: qb}, data={y: y_ph})\n",
    "inference.initialize(\n",
    "    n_iter=n_batch * n_epoch * 10, n_samples=5, scale={y: N / M})\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(inference.n_iter // 10):\n",
    "  X_batch, y_batch = next(data)\n",
    "  for _ in range(10):\n",
    "    info_dict = inference.update({X: X_batch, y_ph: y_batch})\n",
    "\n",
    "  inference.print_progress(info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般的には訓練のイテレーションの総数はinferenceを初期化する際に正しく設定されることに注意してください。\n",
    "あるいは間違った訓練のイテレーション数は意図しない結果をもたらします。\n",
    "例えばed.KLqpは内部にカウンターを持っていてoptimizerの学習率ステップ数を近似的に減衰させて行くために使っています。\n",
    "\n",
    "アルゴリズムを実行して表示されるlossの値はデータセット全体ではなく現在のバッチに対応した目的関数の計算結果であることに注意してください。\n",
    "代わりにエポックごとにinfo_dict['loss']を足し合わせることでデータセット全体のlossの値を得ることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回帰問題の標準的な評価は予測の正確さを取り置いておいた\"テスト\"データと比較することでなされます。\n",
    "\n",
    "ここではまず事後予測分布を作ることでそれを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_post = ed.copy(y, {w: qw, b: qb})\n",
    "# This is equivalent to\n",
    "# y_post = Normal(loc=ed.dot(X, qw) + qb, scale=tf.ones(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで様々な量をモデルを使った予測で評価することができます(事後予測)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error on test data:\n",
      "0.00527082\n",
      "Mean absolute error on test data:\n",
      "0.0507297\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={X: X_test, y_post: y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={X: X_test, y_post: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 脚注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP, KLqp, SGLDといった限られたアルゴリズムのみがバッチ訓練をサポートしています。\n",
    "上にあげたモデルに対するバッチ訓練は全てのデータ点に対して共通の変数である大域的な隠れた変数に対してのみ行われます。\n",
    "さらに複雑なやり方に関しては[ inference data subsampling API](http://edwardlib.org/api/inference-data-subsampling)を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
