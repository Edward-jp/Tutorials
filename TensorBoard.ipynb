{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoardはEdwardのプログラムの理解、デバッグ、最適化を簡単にするための一体化した可視化ツールを提供しています。\n",
    "[tensorflow.org](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard)\n",
    "によると\"TensorFlow graphを可視化したり、グラフを実行した時の定量的な計量をプロットしたり、画像のような付加的なデータを表示するのに使うことができます\"\n",
    "\n",
    "Jupyterでのチュートリアルの原文は[ここ](http://nbviewer.jupyter.org/github/blei-lab/edward/blob/master/notebooks/tensorboard.ipynb)にあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](http://edwardlib.org/images/tensorboard-scalars.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoardを使うにはまず推測中のログを保存するためのディレクトリを指定しなければいけません。例として推測を手動で行う場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.initialize(logdir='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "を呼びます。全てのinference.run()を捕捉しようとする場合logdirを引数に含めます。推測処理が走るとの現在のディレクトリのlog/ 以下に結果が出力されます。\n",
    "コマンドラインではディレクトリを指定してTensorBoardを実行するには"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=log/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "とします。このコマンドはTensorBoardにアクセスするためのwebアドレスを返します。デフォルトではhttp://localhost:6006　です。\n",
    "正しく動いた場合には上記の画像のように見えるでしょう。\n",
    "\n",
    "準備完了です！\n",
    "\n",
    "さらなる手順は　TensorBoardの命名を初期化するために行なわれます。\n",
    "特に計算グラフないのテンソルとランダム変数の名前を設定してすることでしょう\n",
    "完全な例を提供するためにここでは隠れた構造をラベルの付いた例$\\{(x_n, y_n)\\}$から推測する教師あり学習のチュートリアルを拡張します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練と4040個のデータ点をシミュレートします。データは入力$ \\mathbf{x}_n\\in\\mathbb{R}^{5}$と出力$y_n\\in\\mathbb{R}y$です。\n",
    "これらは正規分布のノイズが加わった線形の関係にあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, w):\n",
    "  D = len(w)\n",
    "  x = np.random.normal(0.0, 2.0, size=(N, D))\n",
    "  y = np.dot(x, w) + np.random.normal(0.0, 0.01, size=N)\n",
    "  return x, y\n",
    "\n",
    "ed.set_seed(42)\n",
    "\n",
    "N = 40  # number of data points\n",
    "D = 5  # number of features\n",
    "\n",
    "w_true = np.random.randn(D) * 0.5\n",
    "X_train, y_train = build_toy_dataset(N, w_true)\n",
    "X_test, y_test = build_toy_dataset(N, w_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベイズ線形回帰モデル(Murphy, 2012)を仮定します。\n",
    "モデルではN個のデータ点$(\\mathbf{X},\\mathbf{y})=\\{(\\mathbf{x}_n, y_n)\\}$が以下の分布にしたがっていると仮定します。\n",
    "\n",
    "\\begin{aligned} p(\\mathbf{w}) &= \\text{Normal}(\\mathbf{w} \\mid \\mathbf{0}, \\sigma_w^2\\mathbf{I}), \\\\[1.5ex] p(b) &= \\text{Normal}(b \\mid 0, \\sigma_b^2), \\\\ p(\\mathbf{y} \\mid \\mathbf{w}, b, \\mathbf{X}) &= \\prod_{n=1}^N \\text{Normal}(y_n \\mid \\mathbf{x}_n^\\top\\mathbf{w} + b, \\sigma_y^2).\\end{aligned}\n",
    "\n",
    "線形モデルの隠れた変数は重み$\\mathbf{w}$とバイアスとも呼ばれる切片bです。$\\sigma_w^2,\\sigma_b^2$が事前分布の分散で$ \\sigma_y^2$が既知の尤度の分散であるとします。\n",
    "尤度の意味は入力$\\mathbf{x}_n$の線形変換から与えられます。\n",
    "\n",
    "Edwardでモデルを作りましょう。$\\sigma_w,\\sigma_b,\\sigma_y=1$と固定すると"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('model'):\n",
    "  X = tf.placeholder(tf.float32, [N, D], name=\"X\")\n",
    "  w = Normal(loc=tf.zeros(D, name=\"weights/loc\"),\n",
    "             scale=tf.ones(D, name=\"weights/scale\"),\n",
    "             name=\"weights\")\n",
    "  b = Normal(loc=tf.zeros(1, name=\"bias/loc\"),\n",
    "             scale=tf.ones(1, name=\"bias/scale\"),\n",
    "             name=\"bias\")\n",
    "  y = Normal(loc=ed.dot(X, w) + b,\n",
    "             scale=tf.ones(N, name=\"y/scale\"),\n",
    "             name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでプレースホルダーXを定義しました。推測の間このプレースホルダーにバッチごとのデータの値を渡します。\n",
    "また名前のスコープ(tf.name_scope)を用いました。これはスコープの名前 (\"model/\")をそのwithコンテクスト内の全てのテンソルにプレフィックスとしてつけるものです。\n",
    "全てのランダム変数のパラメータはグループ化された名前システムに基づいて名前づけされることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変分推論を使って事後分布を推測します。\n",
    "変分モデルを重みに関して正規分布で完全に因子分解されたもの指定定義します。そして変分族のグループの名前の別のスコープを加えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"posterior\"):\n",
    "  qw_loc = tf.get_variable(\"qw/loc\", [D])\n",
    "  qw_scale = tf.nn.softplus(tf.get_variable(\"qw/unconstrained_scale\", [D]))\n",
    "  qw = Normal(loc=qw_loc, scale=qw_scale, name=\"qw\")\n",
    "  qb_loc = tf.get_variable(\"qb/loc\", [1])\n",
    "  qb_scale = tf.nn.softplus(tf.get_variable(\"qb/unconstrained_scale\", [1]))\n",
    "  qb = Normal(loc=qb_loc, scale=qb_scale, name=\"qb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カルバックライブラーダイバージェンスを用いて変分推論を走らせます。ここではblack box stochastic gradient\n",
    "を計算するために５つの隠れた変数をサンプルします(詳細は[KL(q||p)のチュートリアル](http://edwardlib.org/tutorials/klqp)を見てください。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})\n",
    "inference.run(n_samples=5, n_iter=250, logdir='log/n_samples_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "オプションとして名前スコープ\"inference\"を含めることもできます。\n",
    "これがないとグラフは分割され一つの\"inference\"自動的にまとめられることはありません。\n",
    "もし\"inference\"が追加されれば計算グラフはより組織立って表示されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoardは任意の問題に対して探索的学習や対話的に使うことができます。\n",
    "上記のいずれかのコマンドでTensorBoardを走らせた後にタブを操作することができます。\n",
    "\n",
    "以下では上記のコードをハイパーパラメーターn_samplesが違う2つの設定で実行したものとします。\n",
    "ログのディレクトリはog/n_samples_です。\n",
    "デフォルトではEdwardは同じ実験の\n",
    "異なる実行に対してタイムスタンプの押されたサブディレクトリを作るためにTensorBoardのログは適切に組織づけられます。\n",
    "\n",
    "推測処理の間　log_timestamp=False　と設定することでこれをoffにすることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Scalars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/tensorboard-scalars.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalarsはアルゴリズムのイテレーションによって変化するスカラー値の情報を総経過時間, 総経過時間の相対値に対して表示します。\n",
    "Edwardではタブにはモデルまたは近似族の中の\n",
    "スカラーのTensorflowの変数(自由なパラメーター)の値が表示されます。\n",
    "\n",
    "\n",
    "変分推論においては損失関数(loss関数)やその個々の項のような情報を表示できます。\n",
    "この例ではn_samples=1の場合にはn_samples=5の場合より高い分散を取りがちであるが同じ解に収束する事が示されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/tensorboard-graphs-0.png)\n",
    "![](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/tensorboard-graphs-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphsはモデル、近似族、推測処理の計算グラフを表示します。\n",
    "(灰色の)箱は同じ名前スコープの中にまとめられたテンソルを意味します。\n",
    "グラフの中の名前を整理することでコードを理解しやすくすることができ、最適化できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/tensorboard-distributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistributionsはイテレーションごとのスカラーでないTensorFlowの変数の分布を表示します。\n",
    "これらの変数はモデルと近似族の自由なパラメータです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard Histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/tensorboard-histograms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HistogramsはDistributionsと同じ情報を３次元ヒストグラムで表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgments（原文)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このチュートリアルの最初のバージョンを書いたSean Kruzelに感謝します。\n",
    "\n",
    "TensorflowのTensorBoardは[ここ](https://www.tensorflow.org/get_started/summaries_and_tensorboard)にあります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Murphy, K. P. (2012). Machine learning: A probabilistic perspective. MIT Press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
